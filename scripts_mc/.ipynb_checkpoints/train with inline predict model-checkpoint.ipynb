{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9c2002b-20d7-4fe1-a587-767bda955d92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marcase/.conda/envs/hgraph-rdkit/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#train with seperate encoder and predictor models bc I can't get transfer learning to work\n",
    "import math, random, sys\n",
    "sys.path.insert(0, '/home/marcase/hgraph2graph/')\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import DataLoader\n",
    "import networkx as nx\n",
    "import rdkit\n",
    "import numpy as np\n",
    "import argparse\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "import pickle\n",
    "from hgraph import *\n",
    "from hgraph.inc_graph import *\n",
    "from hgraph.encoder import *\n",
    "import matplotlib.pyplot as plt\n",
    "from hgraph.predict import HierPredict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1219fd96-4d2c-4e23-bd76-cdacf5d5c785",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C',\n",
       " 'C1=CC=CC=C1',\n",
       " 'C1=CNC=C1',\n",
       " 'C1=CNC=N1',\n",
       " 'C1CCNC1',\n",
       " 'C1CNCCNCCNCCNCCNCCNCCNCCNCCNCCNCCNCCSCCCSC1',\n",
       " 'C1CNCCNCCNCCNCCNCCNCCNCCNCCNCCNCCSCCCSC1',\n",
       " 'C1CNCCNCCNCCNCCNCCNCCNCCNCCNCCSCCCSC1',\n",
       " 'C1CNCCNCCNCCNCCNCCNCCNCCNCCSCCCSC1',\n",
       " 'C1CNCCNCCNCCNCCNCCNCCNCCSCCCSC1',\n",
       " 'C1CNCCNCCNCCNCCNCCNCCNCCSCCCSCCCNCCSCCCSC1',\n",
       " 'C1CNCCNCCNCCNCCNCCNCCNCCSCCCSCCNCCCSCCCSC1',\n",
       " 'C1CNCCNCCNCCNCCNCCNCCSCCCSC1',\n",
       " 'C1CNCCNCCNCCNCCNCCNCCSCCCSCCCNCCNCCSCCCSC1',\n",
       " 'C1CNCCNCCNCCNCCNCCNCCSCCCSCCCNCCSCCCSC1',\n",
       " 'C1CNCCNCCNCCNCCNCCNCCSCCCSCCNCCCSCCCSC1',\n",
       " 'C1CNCCNCCNCCNCCNCCSCCCSC1',\n",
       " 'C1CNCCNCCNCCNCCNCCSCCCSCCCNCCNCCNCCSCCCSC1',\n",
       " 'C1CNCCNCCNCCNCCNCCSCCCSCCCNCCNCCSCCCSC1',\n",
       " 'C1CNCCNCCNCCNCCNCCSCCCSCCCNCCSCCCSC1',\n",
       " 'C1CNCCNCCNCCNCCNCCSCCCSCCNCCCSCCCSC1',\n",
       " 'C1CNCCNCCNCCNCCSCCCSC1',\n",
       " 'C1CNCCNCCNCCNCCSCCCSCCCNCCNCCNCCNCCSCCCSC1',\n",
       " 'C1CNCCNCCNCCNCCSCCCSCCCNCCNCCNCCSCCCSC1',\n",
       " 'C1CNCCNCCNCCNCCSCCCSCCCNCCNCCSCCCSC1',\n",
       " 'C1CNCCNCCNCCNCCSCCCSCCCNCCSCCCSC1',\n",
       " 'C1CNCCNCCNCCNCCSCCCSCCNCCCSCCCSC1',\n",
       " 'C1CNCCNCCNCCNCCSCCCSCCNCCNCCCSCCCSC1',\n",
       " 'C1CNCCNCCNCCSCCCSC1',\n",
       " 'C1CNCCNCCNCCSCCCSCCCNCCNCCNCCSCCCSC1',\n",
       " 'C1CNCCNCCNCCSCCCSCCCNCCNCCSCCCSC1',\n",
       " 'C1CNCCNCCNCCSCCCSCCCNCCSCCCSC1',\n",
       " 'C1CNCCNCCNCCSCCCSCCNCCCSCCCSC1',\n",
       " 'C1CNCCNCCNCCSCCCSCCNCCNCCCSCCCSC1',\n",
       " 'C1CNCCNCCSCCCSC1',\n",
       " 'C1CNCCNCCSCCCSCCCNCCNCCSCCCSC1',\n",
       " 'C1CNCCNCCSCCCSCCCNCCSCCCSC1',\n",
       " 'C1CNCCNCCSCCCSCCNCCCSCCCSC1',\n",
       " 'C1CNCCNCCSCCCSCCNCCNCCCSCCCSC1',\n",
       " 'C1CNCCSCCCSC1',\n",
       " 'C1CNCCSCCCSCCCNCCSCCCSC1',\n",
       " 'C1CNCCSCCCSCCNCCCSCCCSC1',\n",
       " 'C=N',\n",
       " 'C=O',\n",
       " 'CC',\n",
       " 'CN',\n",
       " 'CO',\n",
       " 'CS',\n",
       " 'N']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = '/home/marcase/hgraph2graph/data/chembl/cyclic_peptide_vocab.txt'\n",
    "vocab = [x.strip(\"\\r\\n \").split() for x in open(vocab)]\n",
    "vocab = PairVocab(vocab)\n",
    "vocab.vocab[21][0]\n",
    "vocab.hvocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3df8b4e2-3b4a-4d69-bfcb-5f7afc5715b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    train = '/home/marcase/hgraph2graph/predict/preprocessed_master/preprocessed_train/'\n",
    "    train_labels = '/home/marcase/hgraph2graph/predict/preprocessed_master/preprocessed_train_labels/'\n",
    "    test = '/home/marcase/hgraph2graph/predict/preprocessed_master/preprocessed_test/'\n",
    "    test_labels = '/home/marcase/hgraph2graph/predict/preprocessed_master/preprocessed_test_labels/'\n",
    "    vocab = vocab\n",
    "    save_dir = 'test/'\n",
    "    atom_vocab = common_atom_vocab\n",
    "    load_model = None\n",
    "    seed = 7\n",
    "    rnn_type = 'LSTM'\n",
    "    hidden_size=125\n",
    "    embed_size=250\n",
    "    batch_size=8\n",
    "    latent_size=32\n",
    "    depthT=15\n",
    "    depthG=15\n",
    "    diterT=1\n",
    "    diterG=3\n",
    "    dropout=0.2\n",
    "    lr = 5e-4\n",
    "    clip_norm=5.0\n",
    "    step_beta=0.001\n",
    "    max_beta=1.0\n",
    "    warmup=10000\n",
    "    kl_anneal_iter=2000\n",
    "    epoch=2000\n",
    "    anneal_rate=0.9\n",
    "    anneal_iter=25000\n",
    "    print_iter=50\n",
    "    save_iter=1000000\n",
    "    model = '/home/marcase/hgraph2graph/ckpt/cyclic_peptide_pretrained/model.ckpt.110000'\n",
    "    load_model = True\n",
    "    nsample = 1\n",
    "    label_size = 2\n",
    "    run_test = True\n",
    "    separate_predict = False\n",
    "    \n",
    "args=Args()\n",
    "\n",
    "#get model\n",
    "if args.separate_predict:\n",
    "    encoder = HierVAE(args).cuda()\n",
    "    model = HierPredict(args).cuda()\n",
    "else:\n",
    "    model = HierVAE(args).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "112593a2-9af6-4d00-a310-61cd085f0e1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "continuing from checkpoint /home/marcase/hgraph2graph/ckpt/cyclic_peptide_pretrained/model.ckpt.110000\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(args.seed)\n",
    "random.seed(args.seed)\n",
    "\n",
    "#initialize weights\n",
    "if args.separate_predict:\n",
    "    for param in encoder.parameters():\n",
    "        if param.dim() == 1:\n",
    "            nn.init.constant_(param, 0)\n",
    "        else:\n",
    "            nn.init.xavier_normal_(param)\n",
    "    for param in model.parameters():\n",
    "        if param.dim() == 1:\n",
    "            nn.init.constant_(param, 0)\n",
    "        else:\n",
    "            nn.init.xavier_normal_(param)   \n",
    "else:\n",
    "    for param in model.parameters():\n",
    "        if param.dim() == 1:\n",
    "            nn.init.constant_(param, 0)\n",
    "        else:\n",
    "            nn.init.xavier_normal_(param)        \n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "# scheduler = lr_scheduler.ExponentialLR(optimizer, args.anneal_rate)\n",
    "\n",
    "#load model if args.load_model == True\n",
    "if args.load_model:\n",
    "    print('continuing from checkpoint ' + args.model)\n",
    "    model_state, optimizer_state, total_step, beta = torch.load(args.model)\n",
    "    \n",
    "    if args.separate_predict:\n",
    "        encoder.load_state_dict(model_state)\n",
    "    else:\n",
    "        #initialize weights in model.predict that don't exist from pre-training\n",
    "        for key in model.predict.state_dict().keys():\n",
    "            model_state['predict.' + key] = model.predict.state_dict()[key]\n",
    "        \n",
    "        model.load_state_dict(model_state)\n",
    "else:\n",
    "    total_step = beta = 0\n",
    "\n",
    "param_norm = lambda m: math.sqrt(sum([p.norm().item() ** 2 for p in m.parameters()]))\n",
    "grad_norm = lambda m: math.sqrt(sum([p.grad.norm().item() ** 2 for p in m.parameters() if p.grad is not None]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "745399c9-379c-4ab0-96cb-8949db540d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "meters = np.array([])\n",
    "meters_list = list(meters)\n",
    "validation_list = list()\n",
    "total_step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af96af0d-80e6-4d6f-9b48-be8b6d81844b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50] Beta: 0.050, loss: 0.717, accuracy: 0.500, PNorm: 357.67, GNorm: 1.03\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1781215/2614914952.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mtotal_step\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/hgraph-rdkit/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/hgraph2graph/hgraph/hgnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, graphs, tensors, orders, beta, perturb_z, decode, predict)\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mtree_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_cuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m         \u001b[0mroot_vecs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtree_vecs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph_vecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m         \u001b[0mroot_vecs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroot_kl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrsample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot_vecs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mR_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mR_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperturb_z\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mkl_div\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroot_kl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/hgraph-rdkit/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/hgraph2graph/hgraph/encoder.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tree_tensors, graph_tensors)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0mtensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_inter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhatom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m         \u001b[0mhinter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minter_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0mtensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhinter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/hgraph-rdkit/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/hgraph2graph/hgraph/encoder.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, fnode, fmess, agraph, bgraph)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfmess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfmess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_hidden_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mnei_message\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex_select_ND\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/hgraph-rdkit/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/hgraph2graph/hgraph/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, fmess, bgraph)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdepth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m             \u001b[0mh_nei\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex_select_ND\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m             \u001b[0mc_nei\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex_select_ND\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfmess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_nei\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_nei\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/hgraph2graph/hgraph/nnutils.py\u001b[0m in \u001b[0;36mindex_select_ND\u001b[0;34m(source, dim, index)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0msuffix_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mfinal_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex_size\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msuffix_dim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_select\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#main train loop\n",
    "for epoch in range(args.epoch):\n",
    "    random.seed(args.seed)\n",
    "    dataset_x = DataFolder(args.train, args.batch_size,shuffle = False)\n",
    "    dataset_y = DataFolder(args.train_labels, args.batch_size,shuffle = False)\n",
    "    dataset_x.data_files = ['tensors-'+str(i)+'.pkl' for i in range(len(dataset_x.data_files))]\n",
    "    dataset_y.data_files = ['tensors_labels-'+str(i)+'.pkl' for i in range(len(dataset_y.data_files))]\n",
    "    model.train()\n",
    "    for batch_x,batch_y in zip(dataset_x,dataset_y):\n",
    "        total_step += 1\n",
    "        model.zero_grad()\n",
    "        y_pred = model(*batch_x, beta=beta,decode=False,predict=True) \n",
    "        y_true = torch.Tensor([int(y) for y in batch_y]).cuda()\n",
    "        y_true = y_true.type(torch.LongTensor).cuda()\n",
    "        loss = criterion(y_pred,y_true)\n",
    "        accuracy = torch.sum(torch.argmax(y_pred, dim=1).cuda() == y_true)/len(y_true)\n",
    "        # loss = Variable(loss, requires_grad = True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        meters = np.array([loss.item(),accuracy.cpu()])\n",
    "        meters_list.append(meters)\n",
    "\n",
    "        if total_step % args.print_iter == 0:\n",
    "            print(\"[%d] Beta: %.3f, loss: %.3f, accuracy: %.3f, PNorm: %.2f, GNorm: %.2f\" % (total_step, beta, meters[0], meters[1], param_norm(model), grad_norm(model)))\n",
    "            sys.stdout.flush()\n",
    "            meters *= 0\n",
    "        \n",
    "        if total_step % args.save_iter == 0:\n",
    "            ckpt = (model.state_dict(), optimizer.state_dict(), total_step, beta)\n",
    "            torch.save(ckpt, os.path.join(args.save_dir, f\"model.ckpt.{total_step}\"))\n",
    "\n",
    "        # if total_step % args.anneal_iter == 0:\n",
    "        #     scheduler.step()\n",
    "        #     print(\"learning rate: %.6f\" % scheduler.get_lr()[0])\n",
    "\n",
    "        if total_step >= args.warmup and total_step % args.kl_anneal_iter == 0:\n",
    "            beta = min(args.max_beta, beta + args.step_beta)\n",
    "    \n",
    "    #\"validation\" set\n",
    "    if args.run_test:\n",
    "        model.eval()\n",
    "        dataset_x = DataFolder(args.test, args.batch_size,shuffle = False)\n",
    "        dataset_y = DataFolder(args.test_labels, args.batch_size,shuffle = False)\n",
    "        dataset_x.data_files = ['tensors-'+str(i)+'.pkl' for i in range(len(dataset_x.data_files))]\n",
    "        dataset_y.data_files = ['tensors_labels-'+str(i)+'.pkl' for i in range(len(dataset_y.data_files))]\n",
    "        random.seed()\n",
    "        i=0\n",
    "        accuracy_list = list()\n",
    "        for batch_x,batch_y in zip(dataset_x,dataset_y):\n",
    "            batch_x0 = batch_x\n",
    "            batch_y0 = batch_y\n",
    "            y_pred = model(*batch_x, beta=beta,decode=False,predict=True) \n",
    "            y_pred = torch.argmax(y_pred, dim=1)\n",
    "            y_true = torch.Tensor([int(y) for y in batch_y0]).cuda()\n",
    "            y_true = y_true.type(torch.LongTensor).cuda()\n",
    "            accuracy_list.append((torch.sum(y_pred == y_true)/len(y_pred)).item())\n",
    "            i += 1\n",
    "\n",
    "\n",
    "        print('Accuracy on validation set: %.3f' % np.average(accuracy_list))\n",
    "        validation_list.append((torch.sum(y_pred == y_true)/len(y_pred)).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2312061d-ec50-4c29-aeae-32267aeb6a42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c777879d-4acf-4799-9787-7b20ac5cad9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hgraph-rdkit",
   "language": "python",
   "name": "hgraph-rdkit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

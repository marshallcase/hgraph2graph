{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e54f552-f2a7-43f3-bca8-89abea847cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train with seperate encoder and predictor models bc I can't get transfer learning to work\n",
    "import math, random, sys\n",
    "sys.path.insert(0, '/home/marcase/hgraph2graph/')\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import DataLoader\n",
    "import networkx as nx\n",
    "import rdkit\n",
    "import numpy as np\n",
    "import argparse\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "import pickle\n",
    "from hgraph import *\n",
    "from hgraph.inc_graph import *\n",
    "from hgraph.encoder import *\n",
    "import matplotlib.pyplot as plt\n",
    "from hgraph.predict import HierPredict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e3a62cb-0221-4f45-8d17-5f94673f21b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = '/scratch/gthurber_root/gthurber0/marcase/preprocess_mono/preprocess_generated_real/vocab26oct22.txt'\n",
    "vocab = [x.strip(\"\\r\\n \").split() for x in open(vocab)]\n",
    "vocab = PairVocab(vocab)\n",
    "# vocab.vocab[21][0]\n",
    "# vocab.hvocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3613058a-77d0-4bb1-895d-475b38d1ef42",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    train = '/scratch/gthurber_root/gthurber0/marcase/preprocess_mono/preprocess_generated_real/train/'\n",
    "    train_labels = '/scratch/gthurber_root/gthurber0/marcase/preprocess_mono/preprocess_generated_real/train_labels/'\n",
    "    test = '/scratch/gthurber_root/gthurber0/marcase/preprocess_mono/preprocess_generated_real/test_data/'\n",
    "    test_labels = '/scratch/gthurber_root/gthurber0/marcase/preprocess_mono/preprocess_generated_real/test_labels/'\n",
    "    vocab = vocab\n",
    "    save_dir = 'test/'\n",
    "    atom_vocab = common_atom_vocab\n",
    "    load_model = None\n",
    "    seed = 7\n",
    "    rnn_type = 'LSTM'\n",
    "    hidden_size=250\n",
    "    embed_size=250\n",
    "    batch_size=32\n",
    "    latent_size=32\n",
    "    depthT=15\n",
    "    depthG=15\n",
    "    diterT=1\n",
    "    diterG=3\n",
    "    dropout=0.5\n",
    "    lr = 1e-3\n",
    "    clip_norm=5.0\n",
    "    step_beta=0.001\n",
    "    max_beta=1.0\n",
    "    warmup=10000\n",
    "    kl_anneal_iter=2000\n",
    "    epoch=2000\n",
    "    anneal_rate=0.9\n",
    "    anneal_iter=25000\n",
    "    print_iter=50\n",
    "    save_iter=1000000\n",
    "    model = '/scratch/gthurber_root/gthurber0/marcase/preprocess_mono/preprocess_generated_real/ckpt/pretrained/model.ckpt.340000'\n",
    "    load_model = True\n",
    "    nsample = 1\n",
    "    label_size = 2\n",
    "    max_AA = 15\n",
    "    max_nodes = 100\n",
    "    max_edges = 200\n",
    "    max_sub_nodes = 200\n",
    "    \n",
    "args=Args()\n",
    "\n",
    "encoder = HierVAE(args).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ccc66962-8eab-4300-abca-7c32e7a39562",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HierPredict(\n",
       "  (ff1): Sequential(\n",
       "    (0): Linear(in_features=32, out_features=32, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (ff2): Sequential(\n",
       "    (0): Linear(in_features=32, out_features=32, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (ff3): Sequential(\n",
       "    (0): Linear(in_features=32, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = HierPredict(args).cuda()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3c4eaa9e-c924-4c36-a530-eaaf8bcb1727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "continuing from checkpoint /scratch/gthurber_root/gthurber0/marcase/preprocess_mono/preprocess_generated_real/ckpt/pretrained/model.ckpt.340000\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(args.seed)\n",
    "random.seed(args.seed)\n",
    "\n",
    "for param in encoder.parameters():\n",
    "    if param.dim() == 1:\n",
    "        nn.init.constant_(param, 0)\n",
    "    else:\n",
    "        nn.init.xavier_normal_(param)\n",
    "        \n",
    "for param in model.parameters():\n",
    "    if param.dim() == 1:\n",
    "        nn.init.constant_(param, 0)\n",
    "    else:\n",
    "        nn.init.xavier_normal_(param)        \n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "scheduler = lr_scheduler.ExponentialLR(optimizer, args.anneal_rate)\n",
    "\n",
    "if args.load_model:\n",
    "    print('continuing from checkpoint ' + args.model)\n",
    "    model_state, optimizer_state, total_step, beta = torch.load(args.model)\n",
    "    \n",
    "    encoder.load_state_dict(model_state)\n",
    "\n",
    "else:\n",
    "    total_step = beta = 0\n",
    "\n",
    "param_norm = lambda m: math.sqrt(sum([p.norm().item() ** 2 for p in m.parameters()]))\n",
    "grad_norm = lambda m: math.sqrt(sum([p.grad.norm().item() ** 2 for p in m.parameters() if p.grad is not None]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "78836dcc-d9c0-4239-9652-593fd78a7fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "meters = np.array([])\n",
    "meters_list = list(meters)\n",
    "validation_list = list()\n",
    "total_step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3568ca1b-ce78-4257-8441-3cf4cfc92bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50] Beta: 0.165, loss: 0.970, accuracy: 0.312, PNorm: 8.13, GNorm: 1.74\n",
      "[100] Beta: 0.165, loss: 1.105, accuracy: 0.312, PNorm: 8.05, GNorm: 2.78\n",
      "[150] Beta: 0.165, loss: 0.791, accuracy: 0.500, PNorm: 7.98, GNorm: 1.46\n",
      "[200] Beta: 0.165, loss: 0.759, accuracy: 0.531, PNorm: 7.93, GNorm: 1.05\n",
      "[250] Beta: 0.165, loss: 0.717, accuracy: 0.531, PNorm: 7.89, GNorm: 0.83\n",
      "[300] Beta: 0.165, loss: 0.648, accuracy: 0.594, PNorm: 7.85, GNorm: 1.15\n",
      "[350] Beta: 0.165, loss: 0.695, accuracy: 0.562, PNorm: 7.83, GNorm: 0.70\n",
      "[400] Beta: 0.165, loss: 0.850, accuracy: 0.406, PNorm: 7.80, GNorm: 1.46\n",
      "[450] Beta: 0.165, loss: 0.717, accuracy: 0.531, PNorm: 7.78, GNorm: 0.64\n",
      "[500] Beta: 0.165, loss: 0.709, accuracy: 0.500, PNorm: 7.77, GNorm: 0.60\n",
      "[550] Beta: 0.165, loss: 0.667, accuracy: 0.531, PNorm: 7.75, GNorm: 0.61\n",
      "[600] Beta: 0.165, loss: 0.685, accuracy: 0.594, PNorm: 7.74, GNorm: 0.69\n",
      "[650] Beta: 0.165, loss: 0.742, accuracy: 0.500, PNorm: 7.73, GNorm: 0.74\n",
      "[700] Beta: 0.165, loss: 0.640, accuracy: 0.781, PNorm: 7.72, GNorm: 0.58\n",
      "[750] Beta: 0.165, loss: 0.659, accuracy: 0.656, PNorm: 7.71, GNorm: 0.81\n",
      "[800] Beta: 0.165, loss: 0.721, accuracy: 0.438, PNorm: 7.70, GNorm: 0.85\n",
      "[850] Beta: 0.165, loss: 0.706, accuracy: 0.469, PNorm: 7.70, GNorm: 0.57\n",
      "[900] Beta: 0.165, loss: 0.705, accuracy: 0.562, PNorm: 7.69, GNorm: 0.44\n",
      "[950] Beta: 0.165, loss: 0.725, accuracy: 0.469, PNorm: 7.68, GNorm: 0.71\n",
      "[1000] Beta: 0.165, loss: 0.708, accuracy: 0.344, PNorm: 7.67, GNorm: 0.47\n",
      "[1050] Beta: 0.165, loss: 0.679, accuracy: 0.531, PNorm: 7.66, GNorm: 0.52\n",
      "[1100] Beta: 0.165, loss: 0.699, accuracy: 0.500, PNorm: 7.66, GNorm: 0.58\n",
      "[1150] Beta: 0.165, loss: 0.697, accuracy: 0.562, PNorm: 7.66, GNorm: 0.54\n",
      "[1200] Beta: 0.165, loss: 0.736, accuracy: 0.375, PNorm: 7.65, GNorm: 0.49\n",
      "[1250] Beta: 0.165, loss: 0.711, accuracy: 0.531, PNorm: 7.65, GNorm: 0.49\n",
      "[1300] Beta: 0.165, loss: 0.725, accuracy: 0.438, PNorm: 7.64, GNorm: 0.72\n",
      "[1350] Beta: 0.165, loss: 0.686, accuracy: 0.594, PNorm: 7.64, GNorm: 0.49\n",
      "[1400] Beta: 0.165, loss: 0.705, accuracy: 0.438, PNorm: 7.63, GNorm: 0.42\n",
      "[1450] Beta: 0.165, loss: 0.688, accuracy: 0.500, PNorm: 7.63, GNorm: 0.33\n",
      "[1500] Beta: 0.165, loss: 0.701, accuracy: 0.500, PNorm: 7.62, GNorm: 0.44\n",
      "[1550] Beta: 0.165, loss: 0.710, accuracy: 0.406, PNorm: 7.62, GNorm: 0.46\n",
      "[1600] Beta: 0.165, loss: 0.689, accuracy: 0.531, PNorm: 7.62, GNorm: 0.29\n",
      "[1650] Beta: 0.165, loss: 0.686, accuracy: 0.562, PNorm: 7.61, GNorm: 0.33\n",
      "[1700] Beta: 0.165, loss: 0.701, accuracy: 0.438, PNorm: 7.61, GNorm: 0.41\n",
      "[1750] Beta: 0.165, loss: 0.708, accuracy: 0.312, PNorm: 7.61, GNorm: 0.35\n",
      "[1800] Beta: 0.165, loss: 0.717, accuracy: 0.312, PNorm: 7.61, GNorm: 0.55\n",
      "[1850] Beta: 0.165, loss: 0.688, accuracy: 0.469, PNorm: 7.61, GNorm: 0.35\n",
      "[1900] Beta: 0.165, loss: 0.699, accuracy: 0.469, PNorm: 7.61, GNorm: 0.32\n",
      "[1950] Beta: 0.165, loss: 0.696, accuracy: 0.531, PNorm: 7.60, GNorm: 0.29\n",
      "[2000] Beta: 0.165, loss: 0.687, accuracy: 0.562, PNorm: 7.60, GNorm: 0.23\n",
      "[2050] Beta: 0.165, loss: 0.696, accuracy: 0.438, PNorm: 7.60, GNorm: 0.36\n",
      "[2100] Beta: 0.165, loss: 0.700, accuracy: 0.531, PNorm: 7.59, GNorm: 0.48\n",
      "[2150] Beta: 0.165, loss: 0.700, accuracy: 0.375, PNorm: 7.59, GNorm: 0.42\n",
      "[2200] Beta: 0.165, loss: 0.693, accuracy: 0.500, PNorm: 7.59, GNorm: 0.42\n",
      "[2250] Beta: 0.165, loss: 0.702, accuracy: 0.500, PNorm: 7.59, GNorm: 0.35\n",
      "[2300] Beta: 0.165, loss: 0.695, accuracy: 0.531, PNorm: 7.59, GNorm: 0.32\n",
      "[2350] Beta: 0.165, loss: 0.695, accuracy: 0.500, PNorm: 7.58, GNorm: 0.44\n",
      "[2400] Beta: 0.165, loss: 0.691, accuracy: 0.562, PNorm: 7.58, GNorm: 0.48\n",
      "[2450] Beta: 0.165, loss: 0.678, accuracy: 0.562, PNorm: 7.59, GNorm: 0.38\n",
      "[2500] Beta: 0.165, loss: 0.686, accuracy: 0.500, PNorm: 7.58, GNorm: 0.34\n",
      "[2550] Beta: 0.165, loss: 0.692, accuracy: 0.562, PNorm: 7.58, GNorm: 0.21\n",
      "[2600] Beta: 0.165, loss: 0.689, accuracy: 0.594, PNorm: 7.58, GNorm: 0.29\n",
      "[2650] Beta: 0.165, loss: 0.698, accuracy: 0.500, PNorm: 7.58, GNorm: 0.32\n",
      "[2700] Beta: 0.165, loss: 0.693, accuracy: 0.438, PNorm: 7.57, GNorm: 0.30\n",
      "[2750] Beta: 0.165, loss: 0.699, accuracy: 0.406, PNorm: 7.58, GNorm: 0.27\n",
      "[2800] Beta: 0.165, loss: 0.703, accuracy: 0.438, PNorm: 7.58, GNorm: 0.35\n",
      "[2850] Beta: 0.165, loss: 0.685, accuracy: 0.719, PNorm: 7.58, GNorm: 0.27\n",
      "[2900] Beta: 0.165, loss: 0.682, accuracy: 0.656, PNorm: 7.58, GNorm: 0.49\n",
      "[2950] Beta: 0.165, loss: 0.695, accuracy: 0.406, PNorm: 7.57, GNorm: 0.36\n",
      "[3000] Beta: 0.165, loss: 0.701, accuracy: 0.438, PNorm: 7.57, GNorm: 0.25\n",
      "[3050] Beta: 0.165, loss: 0.687, accuracy: 0.625, PNorm: 7.57, GNorm: 0.51\n",
      "[3100] Beta: 0.165, loss: 0.689, accuracy: 0.656, PNorm: 7.57, GNorm: 0.27\n",
      "[3150] Beta: 0.165, loss: 0.695, accuracy: 0.500, PNorm: 7.57, GNorm: 0.27\n",
      "[3200] Beta: 0.165, loss: 0.690, accuracy: 0.625, PNorm: 7.57, GNorm: 0.36\n",
      "[3250] Beta: 0.165, loss: 0.689, accuracy: 0.594, PNorm: 7.57, GNorm: 0.33\n",
      "[3300] Beta: 0.165, loss: 0.685, accuracy: 0.625, PNorm: 7.56, GNorm: 0.28\n",
      "[3350] Beta: 0.165, loss: 0.694, accuracy: 0.500, PNorm: 7.56, GNorm: 0.23\n",
      "[3400] Beta: 0.165, loss: 0.692, accuracy: 0.562, PNorm: 7.56, GNorm: 0.37\n",
      "[3450] Beta: 0.165, loss: 0.687, accuracy: 0.438, PNorm: 7.56, GNorm: 0.27\n",
      "[3500] Beta: 0.165, loss: 0.693, accuracy: 0.500, PNorm: 7.56, GNorm: 0.32\n",
      "[3550] Beta: 0.165, loss: 0.695, accuracy: 0.406, PNorm: 7.56, GNorm: 0.38\n",
      "[3600] Beta: 0.165, loss: 0.695, accuracy: 0.562, PNorm: 7.56, GNorm: 0.29\n",
      "[3650] Beta: 0.165, loss: 0.700, accuracy: 0.375, PNorm: 7.56, GNorm: 0.40\n",
      "[3700] Beta: 0.165, loss: 0.678, accuracy: 0.625, PNorm: 7.56, GNorm: 0.37\n",
      "[3750] Beta: 0.165, loss: 0.686, accuracy: 0.562, PNorm: 7.56, GNorm: 0.32\n",
      "[3800] Beta: 0.165, loss: 0.700, accuracy: 0.344, PNorm: 7.55, GNorm: 0.32\n",
      "[3850] Beta: 0.165, loss: 0.691, accuracy: 0.469, PNorm: 7.55, GNorm: 0.27\n",
      "[3900] Beta: 0.165, loss: 0.691, accuracy: 0.375, PNorm: 7.55, GNorm: 0.29\n",
      "[3950] Beta: 0.165, loss: 0.700, accuracy: 0.281, PNorm: 7.55, GNorm: 0.33\n",
      "[4000] Beta: 0.165, loss: 0.688, accuracy: 0.531, PNorm: 7.56, GNorm: 0.25\n",
      "[4050] Beta: 0.165, loss: 0.707, accuracy: 0.281, PNorm: 7.55, GNorm: 0.35\n",
      "[4100] Beta: 0.165, loss: 0.688, accuracy: 0.531, PNorm: 7.55, GNorm: 0.22\n",
      "[4150] Beta: 0.165, loss: 0.695, accuracy: 0.406, PNorm: 7.55, GNorm: 0.37\n",
      "[4200] Beta: 0.165, loss: 0.694, accuracy: 0.438, PNorm: 7.55, GNorm: 0.22\n",
      "[4250] Beta: 0.165, loss: 0.705, accuracy: 0.312, PNorm: 7.55, GNorm: 0.28\n",
      "[4300] Beta: 0.165, loss: 0.698, accuracy: 0.625, PNorm: 7.55, GNorm: 0.33\n",
      "[4350] Beta: 0.165, loss: 0.697, accuracy: 0.344, PNorm: 7.55, GNorm: 0.23\n",
      "[4400] Beta: 0.165, loss: 0.692, accuracy: 0.469, PNorm: 7.55, GNorm: 0.23\n",
      "[4450] Beta: 0.165, loss: 0.692, accuracy: 0.562, PNorm: 7.55, GNorm: 0.28\n",
      "[4500] Beta: 0.165, loss: 0.693, accuracy: 0.500, PNorm: 7.55, GNorm: 0.19\n",
      "[4550] Beta: 0.165, loss: 0.695, accuracy: 0.469, PNorm: 7.55, GNorm: 0.18\n",
      "[4600] Beta: 0.165, loss: 0.694, accuracy: 0.438, PNorm: 7.55, GNorm: 0.25\n",
      "[4650] Beta: 0.165, loss: 0.685, accuracy: 0.562, PNorm: 7.55, GNorm: 0.24\n",
      "[4700] Beta: 0.165, loss: 0.696, accuracy: 0.438, PNorm: 7.55, GNorm: 0.24\n",
      "[4750] Beta: 0.165, loss: 0.698, accuracy: 0.406, PNorm: 7.55, GNorm: 0.27\n",
      "[4800] Beta: 0.165, loss: 0.693, accuracy: 0.531, PNorm: 7.55, GNorm: 0.23\n",
      "[4850] Beta: 0.165, loss: 0.700, accuracy: 0.375, PNorm: 7.55, GNorm: 0.22\n",
      "[4900] Beta: 0.165, loss: 0.696, accuracy: 0.469, PNorm: 7.54, GNorm: 0.26\n",
      "[4950] Beta: 0.165, loss: 0.696, accuracy: 0.469, PNorm: 7.54, GNorm: 0.23\n",
      "[5000] Beta: 0.165, loss: 0.694, accuracy: 0.406, PNorm: 7.54, GNorm: 0.28\n",
      "[5050] Beta: 0.165, loss: 0.691, accuracy: 0.625, PNorm: 7.54, GNorm: 0.26\n",
      "[5100] Beta: 0.165, loss: 0.695, accuracy: 0.406, PNorm: 7.54, GNorm: 0.31\n",
      "[5150] Beta: 0.165, loss: 0.685, accuracy: 0.531, PNorm: 7.54, GNorm: 0.25\n",
      "[5200] Beta: 0.165, loss: 0.703, accuracy: 0.312, PNorm: 7.55, GNorm: 0.38\n",
      "[5250] Beta: 0.165, loss: 0.685, accuracy: 0.688, PNorm: 7.56, GNorm: 0.24\n",
      "[5300] Beta: 0.165, loss: 0.694, accuracy: 0.469, PNorm: 7.56, GNorm: 0.30\n",
      "[5350] Beta: 0.165, loss: 0.694, accuracy: 0.469, PNorm: 7.55, GNorm: 0.16\n",
      "[5400] Beta: 0.165, loss: 0.695, accuracy: 0.531, PNorm: 7.55, GNorm: 0.24\n",
      "[5450] Beta: 0.165, loss: 0.692, accuracy: 0.531, PNorm: 7.55, GNorm: 0.27\n",
      "[5500] Beta: 0.165, loss: 0.696, accuracy: 0.531, PNorm: 7.55, GNorm: 0.20\n",
      "[5550] Beta: 0.165, loss: 0.686, accuracy: 0.594, PNorm: 7.55, GNorm: 0.33\n",
      "[5600] Beta: 0.165, loss: 0.709, accuracy: 0.438, PNorm: 7.56, GNorm: 0.32\n",
      "[5650] Beta: 0.165, loss: 0.694, accuracy: 0.375, PNorm: 7.56, GNorm: 0.42\n",
      "[5700] Beta: 0.165, loss: 0.699, accuracy: 0.438, PNorm: 7.56, GNorm: 0.20\n",
      "[5750] Beta: 0.165, loss: 0.695, accuracy: 0.469, PNorm: 7.56, GNorm: 0.15\n",
      "[5800] Beta: 0.165, loss: 0.693, accuracy: 0.562, PNorm: 7.56, GNorm: 0.25\n",
      "[5850] Beta: 0.165, loss: 0.696, accuracy: 0.438, PNorm: 7.57, GNorm: 0.32\n",
      "[5900] Beta: 0.165, loss: 0.695, accuracy: 0.656, PNorm: 7.57, GNorm: 0.36\n",
      "[5950] Beta: 0.165, loss: 0.695, accuracy: 0.469, PNorm: 7.57, GNorm: 0.31\n",
      "[6000] Beta: 0.165, loss: 0.686, accuracy: 0.562, PNorm: 7.58, GNorm: 0.18\n",
      "[6050] Beta: 0.165, loss: 0.690, accuracy: 0.594, PNorm: 7.58, GNorm: 0.21\n",
      "[6100] Beta: 0.165, loss: 0.695, accuracy: 0.469, PNorm: 7.58, GNorm: 0.22\n",
      "[6150] Beta: 0.165, loss: 0.692, accuracy: 0.469, PNorm: 7.58, GNorm: 0.18\n",
      "[6200] Beta: 0.165, loss: 0.699, accuracy: 0.406, PNorm: 7.58, GNorm: 0.21\n",
      "[6250] Beta: 0.165, loss: 0.689, accuracy: 0.500, PNorm: 7.58, GNorm: 0.25\n",
      "[6300] Beta: 0.165, loss: 0.692, accuracy: 0.469, PNorm: 7.59, GNorm: 0.28\n",
      "[6350] Beta: 0.165, loss: 0.691, accuracy: 0.469, PNorm: 7.59, GNorm: 0.19\n",
      "[6400] Beta: 0.165, loss: 0.701, accuracy: 0.344, PNorm: 7.59, GNorm: 0.15\n",
      "[6450] Beta: 0.165, loss: 0.697, accuracy: 0.531, PNorm: 7.60, GNorm: 0.19\n",
      "[6500] Beta: 0.165, loss: 0.697, accuracy: 0.531, PNorm: 7.60, GNorm: 0.23\n",
      "[6550] Beta: 0.165, loss: 0.685, accuracy: 0.594, PNorm: 7.60, GNorm: 0.18\n",
      "[6600] Beta: 0.165, loss: 0.692, accuracy: 0.500, PNorm: 7.61, GNorm: 0.21\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2465949/2635967820.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m# loss = Variable(loss, requires_grad = True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/hgraph-rdkit/lib/python3.7/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    394\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 396\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/hgraph-rdkit/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m def grad(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(args.epoch):\n",
    "    random.seed(args.seed)\n",
    "    dataset_x = DataFolder(args.train, args.batch_size,shuffle = False)\n",
    "    dataset_y = DataFolder(args.train_labels, args.batch_size,shuffle = False)\n",
    "    dataset_x.data_files = ['tensors-'+str(i)+'.pkl' for i in range(len(dataset_x.data_files))]\n",
    "    dataset_y.data_files = ['tensors_labels-'+str(i)+'.pkl' for i in range(len(dataset_y.data_files))]\n",
    "    model.train()\n",
    "    for batch_x,batch_y in zip(dataset_x,dataset_y):\n",
    "        total_step += 1\n",
    "        model.zero_grad()\n",
    "        latent = encoder(*batch_x, beta=beta,decode=False) \n",
    "        y_pred = model(latent)\n",
    "        y_true = torch.Tensor([int(y) for y in batch_y]).cuda()\n",
    "        y_true = y_true.type(torch.LongTensor).cuda()\n",
    "        loss = criterion(y_pred,y_true)\n",
    "        accuracy = torch.sum(torch.argmax(y_pred, dim=1).cuda() == y_true)/len(y_true)\n",
    "        # loss = Variable(loss, requires_grad = True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        meters = np.array([loss.item(),accuracy.cpu()])\n",
    "        meters_list.append(meters)\n",
    "\n",
    "        if total_step % args.print_iter == 0:\n",
    "            print(\"[%d] Beta: %.3f, loss: %.3f, accuracy: %.3f, PNorm: %.2f, GNorm: %.2f\" % (total_step, beta, meters[0], meters[1], param_norm(model), grad_norm(model)))\n",
    "            sys.stdout.flush()\n",
    "            meters *= 0\n",
    "        \n",
    "        if total_step % args.save_iter == 0:\n",
    "            ckpt = (model.state_dict(), optimizer.state_dict(), total_step, beta)\n",
    "            torch.save(ckpt, os.path.join(args.save_dir, f\"model.ckpt.{total_step}\"))\n",
    "\n",
    "        if total_step % args.anneal_iter == 0:\n",
    "            scheduler.step()\n",
    "            print(\"learning rate: %.6f\" % scheduler.get_lr()[0])\n",
    "\n",
    "        if total_step >= args.warmup and total_step % args.kl_anneal_iter == 0:\n",
    "            beta = min(args.max_beta, beta + args.step_beta)\n",
    "    \n",
    "    #\"validation\" set\n",
    "    model.eval()\n",
    "    dataset_x = DataFolder(args.test, args.batch_size,shuffle = False)\n",
    "    dataset_y = DataFolder(args.test_labels, args.batch_size,shuffle = False)\n",
    "    dataset_x.data_files = ['tensors-'+str(i)+'.pkl' for i in range(len(dataset_x.data_files))]\n",
    "    dataset_y.data_files = ['tensors_labels-'+str(i)+'.pkl' for i in range(len(dataset_y.data_files))]\n",
    "    random.seed()\n",
    "    i=0\n",
    "    accuracy_list = list()\n",
    "    for batch_x,batch_y in zip(dataset_x,dataset_y):\n",
    "        batch_x0 = batch_x\n",
    "        batch_y0 = batch_y\n",
    "        latent = encoder(*batch_x, beta=beta,decode=False) \n",
    "        y_pred = model(latent)\n",
    "        y_pred = torch.argmax(y_pred, dim=1)\n",
    "        y_true = torch.Tensor([int(y) for y in batch_y0]).cuda()\n",
    "        y_true = y_true.type(torch.LongTensor).cuda()\n",
    "        accuracy_list.append((torch.sum(y_pred == y_true)/len(y_pred)).item())\n",
    "        i += 1\n",
    "\n",
    "\n",
    "    print('Accuracy on validation set: %.3f' % np.average(accuracy_list))\n",
    "    validation_list.append((torch.sum(y_pred == y_true)/len(y_pred)).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eeb6c4ca-e9b3-49b0-b67b-c900a620ff18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tensors-0.pkl',\n",
       " 'tensors-1.pkl',\n",
       " 'tensors-2.pkl',\n",
       " 'tensors-3.pkl',\n",
       " 'tensors-4.pkl',\n",
       " 'tensors-5.pkl',\n",
       " 'tensors-6.pkl',\n",
       " 'tensors-7.pkl',\n",
       " 'tensors-8.pkl',\n",
       " 'tensors-9.pkl',\n",
       " 'tensors-10.pkl',\n",
       " 'tensors-11.pkl',\n",
       " 'tensors-12.pkl',\n",
       " 'tensors-13.pkl',\n",
       " 'tensors-14.pkl',\n",
       " 'tensors-15.pkl',\n",
       " 'tensors-16.pkl',\n",
       " 'tensors-17.pkl',\n",
       " 'tensors-18.pkl',\n",
       " 'tensors-19.pkl',\n",
       " 'tensors-20.pkl',\n",
       " 'tensors-21.pkl',\n",
       " 'tensors-22.pkl',\n",
       " 'tensors-23.pkl',\n",
       " 'tensors-24.pkl',\n",
       " 'tensors-25.pkl',\n",
       " 'tensors-26.pkl',\n",
       " 'tensors-27.pkl',\n",
       " 'tensors-28.pkl',\n",
       " 'tensors-29.pkl',\n",
       " 'tensors-30.pkl',\n",
       " 'tensors-31.pkl',\n",
       " 'tensors-32.pkl',\n",
       " 'tensors-33.pkl']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_x = DataFolder(args.train, args.batch_size,shuffle = False)\n",
    "dataset_x.data_files = ['tensors-'+str(i)+'.pkl' for i in range(len(dataset_x.data_files))]\n",
    "dataset_x.data_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a107c508-eb8f-4634-8e73-0c575cfcda3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1701, 0.8299],\n",
       "        [0.1210, 0.8790],\n",
       "        [0.4298, 0.5702],\n",
       "        [0.4205, 0.5795],\n",
       "        [0.3272, 0.6728],\n",
       "        [0.1508, 0.8492],\n",
       "        [0.2056, 0.7944],\n",
       "        [0.1318, 0.8682],\n",
       "        [0.2318, 0.7682],\n",
       "        [0.2574, 0.7426],\n",
       "        [0.3937, 0.6063],\n",
       "        [0.3240, 0.6760],\n",
       "        [0.2951, 0.7049],\n",
       "        [0.2564, 0.7436],\n",
       "        [0.3607, 0.6393],\n",
       "        [0.2876, 0.7124],\n",
       "        [0.4141, 0.5859],\n",
       "        [0.2326, 0.7674],\n",
       "        [0.1597, 0.8403],\n",
       "        [0.3544, 0.6456],\n",
       "        [0.2972, 0.7028],\n",
       "        [0.3681, 0.6319],\n",
       "        [0.3508, 0.6492],\n",
       "        [0.3615, 0.6385],\n",
       "        [0.3248, 0.6752],\n",
       "        [0.1423, 0.8577],\n",
       "        [0.1934, 0.8066],\n",
       "        [0.1963, 0.8037],\n",
       "        [0.3888, 0.6112],\n",
       "        [0.2786, 0.7214],\n",
       "        [0.2361, 0.7639],\n",
       "        [0.3717, 0.6283]], device='cuda:0', grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model(latent)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96a56d6-0482-4e9c-862c-4c938cc23c5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hgraph-rdkit",
   "language": "python",
   "name": "hgraph-rdkit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
